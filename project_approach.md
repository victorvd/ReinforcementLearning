# NTN Beam Hopping y Asignaci√≥n de Recursos mediante Aprendizaje Profundo por Refuerzo (DRL)

## El Problema Central

-----

### **Formulaci√≥n Matem√°tica**

Nos enfrentamos a un **problema de optimizaci√≥n estoc√°stica de alta dimensi√≥n**:

$$\maximize‚ÄØ‚ÄØ\mathbb{E}\left[\sum_{t} \gamma^{t} (\alpha\cdot\text{Rendimiento}(t) - \beta\cdot\text{Ca√≠das}(t) + \delta\cdot\text{Equidad}(t) - \eta\cdot\text{Energ√≠a}(t))\right]$$
sujeto a:

$$
\sum_{s=1}^{10} \mathbb{1}_{\{\text{haz}_s \text{ activo}\}} \le 4 \quad \quad \quad \quad \quad (\text{Restricci√≥n de haz}) \\
\sum_{i=1}^{300} \text{PRB}_i \le 51 \quad \quad \quad \quad \quad \quad \quad (\text{Restricci√≥n de recurso}) \\
E_{\text{bater√≠a}}(t) \ge 0 \quad \forall t \quad \quad \quad \quad \quad \quad (\text{Restricci√≥n de energ√≠a}) \\
Q_i(t) \le Q_{\text{max}} \quad \forall i,t \quad \quad \quad \quad \quad \quad (\text{Estabilidad de la cola})
$$

### **Desglose de la Complejidad del Sistema**

| Dimensi√≥n | Complejidad | Por qu√© es Dif√≠cil |
|-----------|------------|---------------|
| **300 Usuarios** | Selecci√≥n combinatoria de usuarios | $2^{300}$ posibles subconjuntos |
| **10 Haces** | Selecci√≥n de haz | $C(10,4) = 210$ patrones |
| **2 Tipos de Tr√°fico** | Programaci√≥n eMBB vs mMTC | Diferentes requisitos de QoS |
| **Canales Variables en el Tiempo** | Din√°mica de la SNR | Entorno no estacionario |
| **Restricciones de Energ√≠a** | Bater√≠a + solar | Compromisos intertemporales |
| **Tr√°fico en R√°fagas** | 5% de probabilidad de r√°faga | Eventos raros pero cr√≠ticos |

## Por Qu√© Fallan los M√©todos Tradicionales

-----

### **Enfoques Cl√°sicos de Optimizaci√≥n**

| M√©todo | Limitaci√≥n Fundamental |
|--------|------------------------|
| **Optimizaci√≥n Convexa** | Requiere un conocimiento futuro perfecto de canales y tr√°fico |
| **Optimizaci√≥n de Lyapunov** | Miope, no puede anticipar eventos de r√°faga |
| **Reglas Heur√≠sticas** | Las pol√≠ticas est√°ticas no se pueden adaptar a condiciones din√°micas |
| **Teor√≠a de Juegos** | Asume agentes racionales independientes |

### **La "Maldici√≥n de la Dimensionalidad"**

$$\text{Tama√±o del Espacio de Estados} = (\text{Estados de Usuario})^{300} \times (\text{Estados de Haz})^{10} \times (\text{Estados de Energ√≠a}) \approx 10^{100}$$
**La programaci√≥n din√°mica tradicional es computacionalmente imposible.**

## Por Qu√© Triunfa el Aprendizaje Profundo por Refuerzo (DRL)

-----

### **El DRL Coincide con las Caracter√≠sticas del Problema**

| Caracter√≠stica del Problema | Ventaja del DRL |
|-----------------|---------------|
| **Estado de alta dimensi√≥n** | Redes neuronales como aproximadores de funciones |
| **Observabilidad parcial** | Aprende de observaciones comprimidas de 40D |
| **Decisiones secuenciales** | Asignaci√≥n de cr√©dito temporal mediante aprendizaje TD |
| **Objetivos m√∫ltiples** | Aprende autom√°ticamente complejos compromisos de recompensa |
| **Din√°mica del sistema desconocida** | Aprendizaje sin modelo a partir de la experiencia |

### **La Formulaci√≥n del Aprendizaje**

$$\text{MDP} = \langle S, A, P, R, \gamma \rangle$$
Donde:
$$S \in \mathbb{R}^{40} \quad (\text{Estado del sistema comprimido})$$
$$A \in [0,10]^{12} \quad (\text{Pesos continuos del haz y del programador})$$
$$P \quad \quad \quad \quad \quad (\text{Desconocido - aprendido de la experiencia})$$
$$R \quad \quad \quad \quad \quad (\text{Funci√≥n de recompensa multi-objetivo})$$
$$\gamma = 0.99 \quad \quad (\text{Factor de descuento para la planificaci√≥n a largo plazo})$$

## Desaf√≠os T√©cnicos Clave que Resuelve el DRL

-----

### **1. Selecci√≥n Adaptativa de Haces bajo Incertidumbre**

$$\text{Pesos}_{\text{Haz}}(t) = f_{\theta}(\text{Cl√∫steres}_{\text{SNR}}(t), \text{Colas}_{\text{Sector}}(t), \text{Energ√≠a}(t))$$

  - **Aprende** qu√© sectores atender bas√°ndose en las condiciones actuales
  - **Anticipa** las r√°fagas de tr√°fico observando los patrones de cola
  - **Equilibra** el rendimiento inmediato frente a las oportunidades futuras

### **2. Asignaci√≥n de Recursos de Servicios M√∫ltiples**

$$\text{Pesos}_{\text{Programador}}(t) = [w_{\text{mmtc}}(t), w_{\text{embb}}(t)]$$

  - **eMBB**: Alta tasa, tolerante a la demora $\to$ pondera la calidad del canal
  - **mMTC**: Baja tasa, sensible a la demora $\to$ pondera las colas pendientes
  - **El DRL aprende** la priorizaci√≥n √≥ptima espec√≠fica del servicio

### **3. Gesti√≥n del Compromiso Energ√≠a-Demora**

$$\text{Costo}_{\text{Energ√≠a}} = \text{Activaci√≥n}_{\text{Haz}} + \text{Potencia}_{\text{Transmisi√≥n}}$$

  - **Aprende** cu√°ndo conservar energ√≠a frente a cu√°ndo gastarla
  - **Se adapta** a los patrones de recolecci√≥n de energ√≠a solar
  - **Equilibra** el servicio inmediato frente a la sostenibilidad a largo plazo

## Por Qu√© Importa Este Problema

-----

### **Impacto en el Mundo Real**

  - **Despliegues NTN 5G/6G**: Starlink, OneWeb, Amazon Kuiper
  - **Comunicaciones de Emergencia**: Respuesta a desastres cuando fallan las redes terrestres
  - **Conectividad Rural**: Reducir la brecha digital a trav√©s de sat√©lite
  - **Conectividad Masiva IoT**: Millones de sensores en √°reas remotas

### **Importancia Econ√≥mica**

  - **Costos operativos de sat√©lite**: $\sim\$1\text{M/a√±o}$ por sat√©lite
  - **Eficiencia del espectro**: Asignaci√≥n limitada de espectro licenciado
  - **Restricciones de energ√≠a**: Necesidad de operaci√≥n con energ√≠a solar

## Brecha de Investigaci√≥n Abordada

-----

### **Limitaciones Actuales en la Literatura**

1.  **Modelos simplificados**: Asumen tr√°fico de b√∫fer completo o canales est√°ticos
2.  **Enfoque de objetivo √∫nico**: Optimizan rendimiento O equidad O energ√≠a
3.  **Optimizaci√≥n centralizada**: Requieren conocimiento global instant√°neo
4.  **Sin adaptaci√≥n**: Pol√≠ticas est√°ticas para entornos din√°micos

### **Nuestra Contribuci√≥n de DRL**

1.  **Modelado realista**: Simulaci√≥n basada en trazas con tr√°fico en r√°fagas
2.  **Multi-objetivo learning**: Optimizaci√≥n de extremo a extremo de objetivos en competencia
3.  **Inteligencia distribuida**: Observaciones locales $\to$ optimizaci√≥n global
4.  **Adaptaci√≥n en l√≠nea**: Aprendizaje continuo a partir de la retroalimentaci√≥n del entorno

## Ventajas Esperadas de la Soluci√≥n DRL

-----

### **Mejoras Cuantitativas**

$$\text{Esperado: DRL vs Mejor Referencia}$$
$$\text{Rendimiento: } \quad +15-20\%$$
$$\text{Ca√≠das de Paquetes: } \quad -70-85\%$$
$$\text{Eficiencia Energ√©tica: } +10-15\%$$
$$\text{Equidad: } \quad \quad +20-30\%$$

### **Ventajas Cualitativas**

  - **Robustez**: Maneja patrones de tr√°fico inesperados
  - **Escalabilidad**: Pol√≠tica √∫nica para densidades de usuario variables
  - **Generalidad**: Se transfiere a diferentes constelaciones de sat√©lites
  - **Autonom√≠a**: No requiere ajuste manual de par√°metros

## Resumen: ¬øPor Qu√© DRL para la Asignaci√≥n de Recursos NTN?

| Aspecto | M√©todos Tradicionales | Soluci√≥n DRL |
|--------|---------------------|--------------|
| **Complejidad del Estado** | Maldici√≥n de la dimensionalidad | Aproximaci√≥n por red neuronal |
| **Compromisos Objetivos** | Ajuste manual de pesos | Aprendizaje autom√°tico de equilibrio |
| **Incertidumbre Ambiental** | Asume estacionariedad | Aprende de datos no estacionarios |
| **Horizonte de Decisi√≥n** | Miope o requiere pron√≥stico | Aprendizaje del valor a largo plazo |
| **Capacidad de Adaptaci√≥n** | Pol√≠ticas est√°ticas | Mejora continua de la pol√≠tica |

**El DRL transforma un problema de optimizaci√≥n irresoluble en una pol√≠tica de control que se puede aprender y que descubre estrategias complejas que los dise√±adores humanos pasar√≠an por alto.**

-----

# Perspectiva de la Inform√°tica: Formulaci√≥n del Problema y Algoritmos

## Clasificaci√≥n del Problema

-----

### **Tipo de Problema Central**

```python
Problema: Proceso de Decisi√≥n de Markov Parcialmente Observable de Alta Dimensi√≥n (POMDP)

Formalmente: ‚ü®S, A, O, P, R, Œ©, Œ≥‚ü© donde:
- S: Espacio de estado del sistema (oculto, alta dimensi√≥n)
- A: Espacio de acci√≥n continuo ‚Ñù¬π¬≤
- O: Espacio de observaci√≥n ‚Ñù‚Å¥‚Å∞ (comprimido, parcial)
- P: Din√°mica de transici√≥n de estado desconocida
- R: Funci√≥n de recompensa multi-objetivo
- Œ©: Funci√≥n de observaci√≥n (mapeo parcial S‚ÜíO)
- Œ≥: Factor de descuento 0.99
```

### **Clase de Complejidad Computacional**

```python
# Este problema exhibe:
NP-Hard:       Selecci√≥n combinatoria de haz (selecci√≥n de subconjunto)
PSPACE:        Toma de decisiones secuencial bajo incertidumbre
#P-Hard:       Razonamiento probabil√≠stico sobre estados ocultos
```

## Formulaci√≥n Abstracta del Problema

-----

### **Como un Problema de Optimizaci√≥n en L√≠nea**

```python
def optimizacion_en_linea(observacion_parcial, historial):
    # Dado: Instant√°nea comprimida del sistema y contexto hist√≥rico
    # Encontrar: Vector de control continuo que maximiza la recompensa esperada a largo plazo
    # Bajo: Observabilidad parcial, restricciones de recursos, incertidumbre
    
    return vector_accion # Control continuo de 12 dimensiones
```

### **Desaf√≠os Computacionales Clave**

#### **1. Maldici√≥n de la Dimensionalidad**

```python
# Los enfoques ingenuos fallan debido a:
tamano_espacio_estado = O(10^40) # ¬°Despu√©s de la compresi√≥n!
tamano_espacio_accion = O(10^12) # Espacio continuo de 12D
```

#### **2. Observabilidad Parcial**

```python
# El problema de "inferencia de estado oculto":
observacion_actual = comprimir(estado_completo_del_sistema) # Compresi√≥n con p√©rdida
# Debe aprender: estado_creencia = P(estado_completo | historial_observacion)
```

#### **3. Compromisos Multi-Objetivo**

```python
# Los objetivos en competencia crean una compleja frontera de Pareto:
objetivos = ['rendimiento', 'equidad', 'energia', 'confiabilidad']
# No hay un √∫nico punto √≥ptimo - debe aprender las preferencias de compromiso
```

## An√°lisis del Espacio de Algoritmos

-----

### **Enfoques Tradicionales de la Inform√°tica y Por Qu√© Fallan**

| Clase de Algoritmo | Por Qu√© Falla |
|----------------|--------------|
| **Programaci√≥n Din√°mica** | Espacio de estados demasiado grande ($10^{40}$ estados) |
| **Programaci√≥n Lineal** | Din√°mica no lineal, restricciones enteras |
| **Algoritmos Gen√©ticos** | Demasiado lentos para tiempo real, baja eficiencia de muestreo |
| **B√∫squeda en √Årbol Monte Carlo** | Espacio de acci√≥n continuo, observabilidad parcial |
| **Q-Learning (tabular)** | Explosi√≥n del espacio estado-acci√≥n |
| **Iteraci√≥n de Pol√≠ticas** | Irresoluble para espacios de alta dimensi√≥n |

### **Por Qu√© Triunfa el Aprendizaje Profundo por Refuerzo (DRL)**

#### **Avance en la Aproximaci√≥n de Funciones**

```python
# El DRL utiliza redes neuronales para aproximar:
Red_Q(s, a) ‚âà Recompensa acumulada esperada # Funci√≥n de valor
Red_Pol√≠tica(s) ‚Üí a # Pol√≠tica directa
```

#### **Eficiencia de Muestreo a trav√©s de la Reproducci√≥n de Experiencia**

```python
# En lugar de:
for episodio in range(millones): # RL sin modelo
    experiencia = ejecutar_episodio()
    actualizar_politica(experiencia)
    
# El DRL utiliza:
buffer_reproduccion.almacenar(experiencia) # Aprende de diversos datos hist√≥ricos
lote = buffer_reproduccion.muestra(256) # Rompe las correlaciones temporales
actualizar_redes(lote) # Aprendizaje estable mediante el uso de lotes
```

## Opciones de Dise√±o del Algoritmo DRL

-----

### **Por Qu√© TD3 en Lugar de Alternativas**

| Algoritmo | Ventajas | Desventajas para Este Problema |
|-----------|------|---------------------|
| **DQN** | RL discreto estable | No puede manejar acciones continuas |
| **DDPG** | Acciones continuas | Sobreestima los valores Q, inestable |
| **PPO** | Estable, buena convergencia | Menos eficiente en el muestreo |
| **SAC** | Estado del arte | Ajuste de hiperpar√°metros complejo |
| **TD3** | **Robusto, aborda la sobreestimaci√≥n** | **Mejor equilibrio para nuestras restricciones** |

### **Innovaciones T√©cnicas de TD3 Aplicadas**

```python
# 1. Cr√≠ticos Gemelos - Reduce el sesgo de sobreestimaci√≥n
Q1, Q2 = redes_criticas(estado, accion)
Q_objetivo = min(Q1, Q2) # Estimaci√≥n conservadora del valor Q

# 2. Actualizaciones de Pol√≠tica Retrasadas - Estabiliza el entrenamiento
if paso % 2 == 0: # Actualiza los cr√≠ticos el doble de veces
    actualizar_criticos()
else:
    actualizar_actor()

# 3. Suavizado de la Pol√≠tica Objetivo - Previene el sobreajuste
accion_objetivo = actor_objetivo(siguiente_estado) + ruido_recortado
```

## Racional del Dise√±o de la Arquitectura Neuronal

-----

### **Red del Actor (Pol√≠tica)**

```python
Input(40) ‚Üí FC(256) ‚Üí ReLU ‚Üí LayerNorm ‚Üí FC(256) ‚Üí ReLU ‚Üí FC(12) ‚Üí Tanh ‚Üí Escala(10)

# Opciones de dise√±o:
# - LayerNorm: Maneja distribuciones de entrada no estacionarias
# - Tanh + Escala: Limita las acciones al rango continuo [0,10]
# - 256 unidades: Capacidad suficiente sin sobreajuste
```

### **Redes Cr√≠ticas (Estimaci√≥n de Valor)**

```python
Estado(40) ‚Üí FC(256) ‚Üí ReLU ‚Üí LayerNorm ‚Üí FC(256) ‚Üí ReLU ‚Üí 
            Concatenar(Acci√≥n(12)) ‚Üí FC(256) ‚Üí ReLU ‚Üí FC(1)

# Por qu√© funciona esto:
# - V√≠as separadas: Comprensi√≥n del estado + evaluaci√≥n de la acci√≥n
# - Fusi√≥n tard√≠a: Acci√≥n evaluada en el contexto de la comprensi√≥n del estado
```

## Perspectiva de la Teor√≠a del Aprendizaje Computacional

-----

### **An√°lisis de la Complejidad del Muestreo**

```python
# L√≠mites te√≥ricos frente a la realidad pr√°ctica:
muestras_teoricas = O(|S|√ó|A|) # Millones para nuestro problema
muestras_practicas = 5000 episodios √ó 200 pasos = 1M muestras

# Por qu√© funciona en la pr√°ctica:
1. Aproximaci√≥n de funciones: Generalizaci√≥n a trav√©s de estados similares
2. Reproducci√≥n de experiencia: Eficiencia de datos a trav√©s de la reutilizaci√≥n
3. Aprendizaje por transferencia: Las pol√≠ticas se generalizan a trav√©s de escenarios
```

### **Garant√≠as de Convergencia**

```python
# Bajo condiciones ideales:
- TD3 converge a un √≥ptimo local con probabilidad 1
- La mejora de la pol√≠tica es monot√≥nica bajo ciertas suposiciones

# En la pr√°ctica:
- Se requiere un ajuste cuidadoso de los hiperpar√°metros
- La conformaci√≥n de la recompensa es cr√≠tica para la convergencia
- El equilibrio exploraci√≥n-explotaci√≥n es esencial
```

## Familias de Algoritmos Alternativos

-----

### **Enfoque Jer√°rquico de RL**

```python
def solucion_jerarquica():
    controlador_alto_nivel() # Selecci√≥n de haz (cada 10 pasos)
    programador_nivel_medio() # Priorizaci√≥n de usuarios (a cada paso)
    asignador_nivel_bajo() # Distribuci√≥n de recursos (a cada paso)
```

### **Extensi√≥n de Meta-Aprendizaje**

```python
# Aprender a adaptarse r√°pidamente a nuevos escenarios:
politica_base = entrenar_en_diversos_entornos()
def adaptar_politica(nuevo_entorno, pocas_muestras):
    return ajuste_fino(politica_base, nuevo_entorno, pocas_muestras)
```

### **Formulaci√≥n de RL Multi-Agente**

```python
# Soluci√≥n distribuida:
agentes_haz = [Agente_RL() for _ in range(10)] # Cada haz aut√≥nomo
coordinador = Agente_RL() # Coordinador global de recursos
```

## Por Qu√© Este Es un Problema de Referencia para RL

-----

### **Propiedades Deseables de Referencia de RL**

```python
propiedades = {
    'no_trivial': True, # No puede ser resuelto por reglas simples
    'dificultad_ajustable': True, # Se puede ajustar la carga, las restricciones
    'metricas_claras': True, # Rendimiento, ca√≠das, energ√≠a, equidad
    'observabilidad_parcial': True, # Estado oculto realista
    'acciones_continuas': True, # Control de alta dimensi√≥n
    'multi_objetivo': True, # Objetivos en competencia
    'relevancia_mundo_real': True, # Aplicaciones pr√°cticas
    'reproducible': True # Entorno controlado
}
```

## Ideas Clave de la Inform√°tica

-----

### **La Contribuci√≥n Central de la Inform√°tica**

Este problema demuestra c√≥mo **la aproximaci√≥n de funciones profunda + la reproducci√≥n de experiencia + el dise√±o cuidadoso de algoritmos** pueden resolver problemas que son:

  - **Demostrablemente irresolubles** para algoritmos tradicionales
  - **De alta dimensi√≥n** tanto en el espacio de estados como de acciones
  - **Parcialmente observables** con din√°micas complejas
  - **Multi-objetivo** con metas en competencia

### **Patr√≥n de Innovaci√≥n Algor√≠tmica**

```python
# Receta exitosa para problemas de control dif√≠ciles:
1. Formalizar como MDP/POMDP
2. Elegir el algoritmo DRL apropiado (TD3 para control continuo)
3. Dise√±ar una arquitectura neuronal que coincida con la estructura del problema
4. Implementar la reproducci√≥n de experiencia para la eficiencia de muestreo
5. Conformar cuidadosamente la funci√≥n de recompensa para el comportamiento deseado
6. Ajustar los hiperpar√°metros mediante experimentaci√≥n sistem√°tica
```

Esto representa la **vanguardia de lo que es computacionalmente factible** para problemas complejos de control del mundo real utilizando las metodolog√≠as actuales de aprendizaje profundo por refuerzo.

-----

# üéØ M√©todo DRL Agrupado Jer√°rquico: Explicado

**Nota:** Este enfoque utiliza un **agrupamiento n√≠tido (crisp clustering)** para la ingenier√≠a de caracter√≠sticas y una **arquitectura de decisi√≥n jer√°rquica** para la escalabilidad, lo que lo clasifica como **DRL Agrupado Jer√°rquico**. No incorpora l√≥gica difusa formal (fuzzy logic).

## üèóÔ∏è **La Arquitectura Jer√°rquica**

### **Jerarqu√≠a de Decisi√≥n de Tres Niveles**

```
Nivel 3: Ingenier√≠a de Caracter√≠sticas Basada en Agrupamiento (Estrat√©gico)
    ‚Üì
Nivel 2: Selecci√≥n de Haz a Nivel de Sector (T√°ctico)  
    ‚Üì
Nivel 1: Asignaci√≥n de Recursos a Nivel de Usuario (Operacional)
```

## üîç **Procesamiento de Agrupamiento a Nivel de Grupo**

### **Agrupamiento de Usuarios (Clustering)**

```matlab
function obs = buildObservation(obj)
    % AGRUPAMIENTO N√çTIDO (CRISP CLUSTERING): Usuarios agrupados en 5 grupos n√≠tidos
    for u = 1:obj.numUsers
        c_id = obj.getClusterForUser(u, currentSNR(u)); % Asignaci√≥n n√≠tida
        % Cada usuario pertenece exactamente a un grupo basado en umbrales de SNR y estado de cola
    end
```

### **Definiciones de Grupos N√≠tidos (Cluster Definitions)**

```matlab
% Grupo 1: mMTC-Cr√≠tico (Cola alta, SNR baja)
% Grupo 2: mMTC-Normal (Cola baja, cualquier SNR)  
% Grupo 3: eMBB-Excelente (SNR alta)
% Grupo 4: eMBB-Bueno (SNR media)
% Grupo 5: eMBB-Pobre (SNR baja)

% Asignaci√≥n N√≠tida: Cada usuario tiene un grado de pertenencia de 1.0 a un grupo y 0.0 a los dem√°s.
% Ejemplo: Usuario con SNR=10, Cola=3MB ‚Üí [1.0, 0.0, 0.0, 0.0, 0.0] si cae en el umbral del Grupo 1.
```

## üéØ **El Flujo de Decisi√≥n Jer√°rquico**

### **Nivel 3: Estrategia Basada en Grupos (Preprocesamiento)**

```matlab
% Entrada: Estad√≠sticas de grupo (6 m√©tricas √ó 5 grupos)
% Procesamiento: Las estad√≠sticas comprimen la observaci√≥n de 300 usuarios a un vector de 40D
% Salida: Estado comprimido de entrada para el agente DRL

cluster_stats = [
    avg_SNR_cluster1, avg_queue_cluster1, user_count_cluster1, ...
    avg_SNR_cluster5, avg_queue_cluster5, user_count_cluster5,
    energy_level, solar_input, total_system_load
]
```

### **Nivel 2: Selecci√≥n a Nivel de Sector (DRL T√°ctico)**

```matlab
% Entrada: Estado comprimido (40D)
% Procesamiento: El agente DRL calcula los pesos del haz (acci√≥n)
% Salida: Qu√© 4 sectores activar

sectorWeights = action(1:10); % El DRL aprende patrones √≥ptimos de activaci√≥n de haz
[~, activeSectors] = maxk(sectorWeights, obj.maxBeams); % Selecciona los 4 primeros
```

### **Nivel 1: Asignaci√≥n a Nivel de Usuario (Programador de Reglas Operacionales)**

```matlab
% Entrada: Usuarios activos + pesos de programador (w_mmtc, w_embb) del DRL
% Procesamiento: Programador basado en reglas deterministas
% Salida: Asignaci√≥n de PRB a usuarios individuales

w_mmtc = action(11); % DRL: Peso para el servicio mMTC
w_embb = action(12); % DRL: Peso para el servicio eMBB

% La prioridad de usuario se calcula con una f√≥rmula n√≠tida que combina los pesos del DRL con los datos de usuario (cola o SNR)
for each user in active_sectors:
    if user.service == mMTC:
        priority = w_mmtc * user.queue_size % Prioridad lineal
    else:
        priority = w_embb * log(1 + user.queue_size) * user.snr % Prioridad proporcional a tasa
```

## üéØ **Clasificaci√≥n Correcta del Problema**

**La implementaci√≥n actual es:**

```python
Problema: Aprendizaje Profundo por Refuerzo Jer√°rquico Agrupado
(Hierarchical Clustered Deep Reinforcement Learning)

T√©cnicas Clave:
1. Compresi√≥n del Espacio de Estados a trav√©s de Agrupamiento N√≠tido (Clustering)
2. Arquitectura de Decisi√≥n Jer√°rquica  
3. Asignaci√≥n de Recursos a M√∫ltiples Escalas de Tiempo
4. Control H√≠brido Basado en Reglas + Basado en Aprendizaje
```

### **Fortalezas del M√©todo DRL Agrupado Jer√°rquico**

| Aspecto | DRL Agrupado Jer√°rquico |
|---------------------|----------------------------------------------------------|
| **Complejidad del Estado** | Compresi√≥n de 300 usuarios a **5 res√∫menes de grupo (clusters)** |
| **Escalabilidad** | La pol√≠tica √∫nica opera en res√∫menes, no en usuarios individuales |
| **Arquitectura** | Descomposici√≥n de la decisi√≥n en capas **Estrat√©gica (Grupo) y T√°ctica (Haz)** |
| **Control H√≠brido** | El DRL aprende los pesos ($w_{mmtc}, w_{embb}$), el programador usa reglas fijas |

### **La Innovaci√≥n Clave**

La arquitectura combina la **capacidad de aprendizaje y adaptaci√≥n del DRL** con la **escalabilidad** proporcionada por la representaci√≥n del estado basada en el **agrupamiento n√≠tido**. Esto transforma un problema irresoluble de $10^{100}$ estados en un problema de $40$ dimensiones que se puede aprender.

**La implementaci√≥n actual es un sofisticado DRL jer√°rquico con ingenier√≠a de caracter√≠sticas inteligente, no un sistema formal de l√≥gica difusa.**
